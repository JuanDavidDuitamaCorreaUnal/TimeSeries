---
title: "Proyecto Series de Tiempo Univariadas"
format: html
editor: visual
lang: es
---

El siguiente trabajo abarca todas las metodologías vistas en el semestre 2024-1 de Series de Tiempo Univariadas, utilizando la medición de la tasa de desempleo en Colombia; desde enero de 2001 hasta diciembre de 2023.

## Integrantes

-   Ander Steven Cristancho Sánchez
-   John Anderson Guarín López
-   Juan David Duitama Correa

## Librerías utilizadas

```{r}
#| messages: FALSE
#| warning: FALSE
library(plotly)
library(forecast)
library(MASS)
library(tidyverse)
library(lubridate)
library(timetk)
library(tibble)
library(zoo)
library(tsibble)
library(feasts)
library(fable)
library(cowplot)
library(astsa)
library(TSstudio)
library(fabletools)
library(TSA)
library(dplyr)
library(parsnip)
library(rsample)
library(modeltime)
library(tidymodels)
library(readxl)
library(lmtest)
library(tseries)
library(urca)
library(uroot)
library(fUnitRoots)
library(aTSA)
library(sarima)
library(tsoutliers)
library(fpp)
```

## Importación de datos

```{r}
#setwd("D:/Estadística/Semestres/Semestre 9/Series de Tiempo Univariadas/Datos")
#Data <- read.csv("DesempleoLimpio")[,c(2,4)]
#colnames(Data)<-c("AnioMes","TasaDesempleo")
setwd("C:/Users/EQUIPO/Documents/GitHub/TimeSeries/Proyecto/Desempleo")#Ander
Data <- read_excel("Desempleo.xlsx", skip = 5, n_max = 276)
Data <- subset(Data, select = -`Tasa de ocupación (%)`)
#setwd("D:/Estadística/Semestres/Semestre 9/Series de Tiempo Univariadas/Datos")
#Data <- read.csv("DesempleoLimpio")[,c(2,4)]
colnames(Data)<-c("AnioMes","TasaDesempleo")
```

## Visualización de datos

En un primer vistazo podemos notar el pico presente en el periodo concerniente a la pandemia, antes de este periodo existía una tendencia decreciente, a partir del año 2022 los valores de la tasa de desempleo parecen volver a ser similares a aquellos dados previos a la pandemia.

```{r}
DesempleoTS <- ts(rev(Data$TasaDesempleo), start = c(2001, 1), end = c(2023, 12), frequency = 12)
plot_ly( x = time(DesempleoTS), y = DesempleoTS, type = 'scatter', mode = 'lines',color=I("red")) %>%
  layout(title = 'Tasa de Desempleo en Colombia',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))


```

### Autocorrelación

Un primer gráfico de la autocorrelación, recordemos que no es válido hasta que retiremos la tendencia y la estacionalidad.

```{r}
acf(DesempleoTS,lag.max = length(DesempleoTS))
```

## Análisis Descriptivo

En principio la serie aparenta tener un ciclo(estacionalidad) que se repite cada año, es decir, una serie con periodo doce, en la que el mayor pico que se tiene en cada año se encuentra localizado en el mes de enero, exceptuando el 2005 y la pandemia de 2020. En todos los demás años la mayor tasa de desempleo que se reportó ocurrió en enero, y en los meses posteriores fue reduciendo hasta crecer nuevamente finalizando el año. Parece haber una ligera tendencia decreciente desde el año 2001 hasta finales de 2019. El rango de los valores de la tasa de desempleo parece ser constante a simple vista, nuevamente exceptuando la pandemia.

### Estabilización de la Varianza (Tranformación Box-Cox)

A pesar de que el rango de valores de la tasa de desempleo en cada año parece ser constante, es decir, aparenta tener varianza constante, se realizará una transformación de los valores con el fin de evitar problemas futuros; en especial para el periodo posterior al 2019, cuyos valores se vieron afectados por el suceso ya conocido; Covid 19.

```{r}
Lambda<-BoxCox.lambda(DesempleoTS, method ="loglik", lower = -3, upper = 3)#Encontrando el valor de Lambda
Lambda
DesempleoTSBox<-BoxCox(DesempleoTS,lambda = Lambda)#Transformando los datos
```

```{r}
MASS::boxcox(lm(DesempleoTS ~ 1),seq(-3, 3, length = 50))
```

La estimación de Lambda indica que sí es necesario realizar una transformación, además, el intervalo de confianza para este Lambda no incluye el 0.

No hemos retirado la tendencia, solo se han transformado los datos, por lo tanto, no se espera un cambio notorio, a excepción de la reducción del pico de la pandemia, se presenta nuevamente el gráfico de la autocorrelación con la serie transformada.

```{r}
plot_ly( x = time(DesempleoTSBox), y = DesempleoTSBox, type = 'scatter', mode = 'lines',color=I("red")) %>%
  layout(title = 'Tasa de Desempleo en Colombia Transformada',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))
acf(DesempleoTSBox,lag.max = length(DesempleoTSBox))
```

Aunque reduce el pico de la pandemia, ahora parece que la varianza no es constante, pues se ve al comienzo de la serie una menor varianza mientras que en los últimos años la varianza es mucho mayor, por lo que no se trabajará con la serie transformada.

### Extracción de la tendencia

#### Estimación mediante un modelo lineal

Se realiza un modelo de regresión lineal simple utilizando como variable regresora el tiempo, y como variable de estudio, la tasa de desempleo mensual. Por la naturaleza de los datos un modelo lineal no es adecuado, ya que la pandemia fue un evento singular no recreable con una recta, una recta sí podría ser adecuada para el periodo previo a la pandemia.

```{r}
summary(fitLM <- lm(DesempleoTS~time(DesempleoTS), na.action=NULL))#Creando un modelo de regresión lineal
plot(DesempleoTS, ylab="Tasa de Desempleo",xlab="Mes") 
abline(fitLM,col = "red")

```

La serie resultante de extraer la tendencia lineal no aparenta ser una serie que oscile sobre un solo valor. Notorio, reiterando nuevamente; en el período de la pandemia.

```{r}
DesempNoLM=DesempleoTS-predict(fitLM)#Eliminando la tendencia
plot_ly( x = time(DesempNoLM), y = DesempNoLM, type = 'scatter', mode = 'lines',color=I("red")) %>%
  layout(title = 'Tasa de Desempleo en Colombia con tendencia lineal extraída',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))
```

#### Estimación no paramétrica

La estimación no paramétrica permite capturar comportamientos "irregulares", no obstante, reiterando con el periodo de la pandemia, la estimación de la tendencia en este caso requiere que el suavizado sea más sensible. Particularmente se realizó un suavizado local con el 20% de los datos.

```{r}
indice1=as.Date(as.yearmon(tk_index(DesempleoTS)))#Convirtiendo en fecha el indice de DesempleoTS, deja como primero de enero cada fecha
indice1.1=yearmon(as.yearmon(tk_index(DesempleoTS)))#Dejando esa fecha solo como año y mes

## Haciendo el objeto tsibble
df=data.frame(Fecha=indice1,TransDesemp=as.matrix(DesempleoTS))
DesempTibbleTS=as_tsibble(df)

####Primera aproximación del ajuste STL

df%>%timetk::plot_time_series(Fecha, TransDesemp, 
                   .interactive = TRUE,
                   .plotly_slider = TRUE)

#Creando una nueva columna con los datos ajustados por una regresión STL de grado 2 y utilizando el 20% de los datos, entre más pequeño, la curva es menos suave

###Ajuste STL moviendo los parámetros
df%>%mutate(TransDesemp_Mod=smooth_vec(TransDesemp,span = 0.2, degree = 2))%>%
  ggplot(aes(Fecha, TransDesemp)) +
    geom_line() +
    geom_line(aes(y = TransDesemp_Mod), color = "red")
```

La serie de tiempo resultante de extraer la tendencia por métodos no paramétricos sí parece oscilar alrededor de 0, a comparación de la serie anterior, esta serie es adecuada para seguir con los análisis posteriores.

```{r}
STLextra<-DesempleoTS-smooth_vec(DesempleoTS,span = 0.2, degree = 2)
plot_ly( x = time(STLextra), y = STLextra, type = 'scatter', mode = 'lines',color=I("red")) %>%
  layout(title = 'Tasa de Desempleo con tendencia STL extraída',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))
```

El siguiente código permite realizar la descomposición de la serie de tiempo en todas las partes, tendencia, estacionalidad y error, note que la componente de error sigue teniendo un pico muy alto en los meses en que se desarrolló la pandemia.

```{r}
tsibbleDesem<-as_tsibble(DesempleoTS)#Creando el objeto tsibble de la tasa de Desempleo, note que tiene periodo 1M

tsibbleDesem %>%
  model(
    STL(value ~ trend() +
                   season(window = "periodic"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

#### Diferencia ordinaria

El método de diferencia ordinaria, además de ser simple, dio un buen resultado puesto que la serie resultante parece oscilar alrededor de 0, el único dato que se pierde es el que corresponde a Enero de 2001.

```{r}
###Diferenciando con base en el objeto ts

DiferenciaOrd<-diff(DesempleoTS)
plot_ly( x = time(DiferenciaOrd), y = DiferenciaOrd, type = 'scatter', mode = 'lines',color=I("red")) %>%
  layout(title = 'Tasa de Desempleo sin tendencia(Diferencia Ordinaria, lag=1)',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))
#plot(dlAirPass)

```

#### Promedio móvil

Finalmente, con el promedio móvil también se obtuvo una serie con valores oscilantes alrededor de 0, no obstante, utilizando este método se pierden las primeras y últimas 6 observaciones, es decir, Enero, Febrero, Marzo, Abril, Mayo y Junio del 2001, y Julio, Agosto, Septiembre, Octubre, Noviembre, Diciembre del 2023 no tienen estimación.

```{r}
DescomProm=decompose(DesempleoTS)
plot(DescomProm)
ExtraProm=DesempleoTS-DescomProm$trend
plot_ly( x = time(ExtraProm), y = ExtraProm, type = 'scatter', mode = 'lines',color=I("red")) %>%
  layout(title = 'Tasa de Desempleo con tendencia extraída (Promedio móvil)',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))
```

#### Comparando las funciones de autocorrelación de las series sin tendencia

```{r}
acf(DesempleoTS,lag.max = length(DesempleoTS))

par(mfrow = c(2, 2))
acf(DesempNoLM,lag.max =100,main="Autocorrelación serie sin tendencia lineal")
acf(STLextra,lag.max =100,main="Autocorrelación serie sin tendencia STL")
acf(DiferenciaOrd,lag.max =100,main="Autocorrelación serie sin tendencia\npor diferencia ordinaria")
acf(ExtraProm[7:270],lag.max =100,main="Autocorrelación serie sin tendencia\npor promedio móvil")

```

A partir de ahora los análisis próximos serán realizados en todas las series con tendencia extraída, a excepción de la lineal; esto ya que los resultados en el gráfico de autocorrelación no son buenos.

### Explorando relaciones lineales

#### Gráficos de retardos

El gráfico de retardos indica que hay una relación lineal con el retardo número 12 en las 3 series con tendencia extraída.

```{r}
#El gráfico de retardos se realiza con la serie con tendencia extraída

#Tendencia extraída por diferencia ordinaria
par(mar = c(3,2,3,2))
lag1.plot(DiferenciaOrd, 12,corr=T)

```

Del gráfico de retardos de la diferencia ordinaria, se seleccionan los retardos 6, 9, 10 y 12.

#### Gráfico de autocorrelación parcial

```{r}
pacf(DiferenciaOrd, 12)
```

La autocorrelación parcial indica que los retardos 3, 6, 8 y 9 no parecen aportar información relevante.

### Extracción de estacionalidad

Un primer gráfico propuesto para la detección de la estacionalidad de forma visual es el siguiente, recordemos que está siendo aplicado en la serie con la tendencia extraída.

```{r}
#Tendencia extraída por diferencia ordinaria
TSstudio::ts_heatmap(DiferenciaOrd,title = "Mapa de Calor - Tasa de Desempleo en Colombia(Diferencia Ordinaria)")
```

El mapa de calor indica que los valores más altos en la tasa de desempleo fueron reportados en el mes de enero.

En el siguiente gráfico se utilizan las subseries por cada mes, se aprecia más claramente la anterior aseveración, enero es el mes que suele tener las tasas de desempleo más altas.

```{r}
monthplot(DiferenciaOrd,main="Subseries(Diferencia Ordinaria)")
```

Otra alternativa es utilizar los gráficos de cajas.

```{r}
tsibbleDesem <- tsibbleDesem %>%
  mutate(index = as.Date(index))
tsibbleDesem<-tsibbleDesem%>%mutate(Diferencia=value-lag(value))
tsibbleDesem %>%
  na.omit() %>%
  plot_seasonal_diagnostics(.date_var = index, .value = Diferencia, .feature_set = c("month.lbl"), .geom = "boxplot")

```

Finalmente, un gráfico con las estimaciones de las densidades.

```{r}
ggplot(tsibbleDesem %>%na.omit()|>
    mutate(
        Mes = str_c("Mes ", as.character(lubridate::month(index)))
    ), aes(x = Diferencia)) +
      geom_density(aes(fill = Mes)) +
      ggtitle("LosPass - Estimación de la densidad vía Kernel por mes") +
      facet_grid(rows = vars(as.factor(Mes)))
```

#### Encontrando el periodo de la serie

El periodograma concluye que el periodo es 3, sin embargo vemos que hay picos en 0.25, 0.16 y uno más pequeño en 0.083, estas frecuencias representan los periodos 4, 6 y 12 respectivamente. Sabemos que la serie es mensual, por lo que no es raro que el periodograma indique divisores del verdadero periodo, que es 12.

```{r}
#Diferencia ordinaria
PeriodgramadTra3=spectrum(as.numeric(DiferenciaOrd),log='no')
ubicacion3=which.max(PeriodgramadTra3$spec)

sprintf("El valor de la frecuencia donde se máximiza el periodograma es: %s",PeriodgramadTra3$freq[ubicacion3])

sprintf("El periodo correspondiente es aproximadamente: %s",1/PeriodgramadTra3$freq[ubicacion3])

```

#### Estimación de la estacionalidad

```{r}

DiferTSibl<-as_tsibble(DiferenciaOrd)
DiferTSibl

###Explore diferentes valores de K
Modelo_serie_diff<-DiferTSibl|>model(
  `Fourier1Desempleo`=ARIMA(value~fourier(K=4)+pdq(0, 0, 0) + PDQ(0, 0, 0))#En este caso se hace con tres componentes de Fourier
  
)

real_ajustado1<-DiferTSibl%>%left_join(fitted(Modelo_serie_diff,by=index))#%>%select(-.model) 

plot_ly(x = time(DiferenciaOrd), y = DiferenciaOrd, type = 'scatter', mode = 'lines', color = I("red"), name = "Serie1") %>%
  add_trace(y = real_ajustado1$.fitted, mode = 'lines', line = list(color = 'blue'), name = "Ajuste Armónico") %>%
  layout(title = 'Tasa de Desempleo sin tendencia (Diferencia Ordinaria, lag=1)',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))

#####Ajuste Dummy

ModeloDummy<-DiferTSibl|>model(
  `DummyDesempleoDiff`=ARIMA(value~season()+pdq(0, 0, 0) + PDQ(0, 0, 0))
  
)
real_ajustado2<-DiferTSibl%>%left_join(fitted(ModeloDummy,by=index))#%>%select(-.model) 


plot_ly(x = time(DiferenciaOrd), y = DiferenciaOrd, type = 'scatter', mode = 'lines', color = I("red"), name = "Serie1") %>%
  add_trace(y = real_ajustado2$.fitted, mode = 'lines', line = list(color = 'blue'), name = "Ajuste Dummy") %>%
  layout(title = 'Tasa de Desempleo sin tendencia (Diferencia Ordinaria, lag=1)',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))

#### Varios modelos al mismo tiempo

ajuste_final_models<-DiferTSibl%>%model(
 `Fourier1DesempleoDiff`=ARIMA(value~fourier(K=1)+pdq(0, 0, 0) + PDQ(0, 0, 0)),#1 componente de Fourier
 `Fourier2DesempleoDiff`=ARIMA(value~fourier(K=2)+pdq(0, 0, 0) + PDQ(0, 0, 0)),#2 componente de Fourier
 `Fourier3DesempleoDiff`=ARIMA(value~fourier(K=3)+pdq(0, 0, 0) + PDQ(0, 0, 0)),#3 componente de Fourier
  `Fourier4DesempleoDiff`=ARIMA(value~fourier(K=4)+pdq(0, 0, 0) + PDQ(0, 0, 0)),#4 componente de Fourier
`DummyDesempleoDiff`=ARIMA(value~season()+pdq(0, 0, 0) + PDQ(0, 0, 0))#Ajuste dummy
                                        )

glance(ajuste_final_models)

Modelo_serie_diff_models<-DiferTSibl%>%left_join(fitted(ajuste_final_models)|>group_by(.model)%>%
    pivot_wider(names_from = .model, values_from = .fitted))

plot_ly(x = time(DiferenciaOrd), y = DiferenciaOrd, type = 'scatter', mode = 'lines', color = I("red"), name = "SerieDifOrd") %>%
add_trace(y = Modelo_serie_diff_models$Fourier1DesempleoDiff, mode = 'lines', line = list(color = 'blue'), name = "Fourier1") %>%
add_trace(y = Modelo_serie_diff_models$Fourier2DesempleoDiff, mode = 'lines', line = list(color = 'green'), name = "Fourier2") %>%
add_trace(y = Modelo_serie_diff_models$Fourier3DesempleoDiff, mode = 'lines', line = list(color = 'yellow'), name = "Fourier3") %>%
add_trace(y = Modelo_serie_diff_models$Fourier4DesempleoDiff, mode = 'lines', line = list(color = '#00CD66'), name = "Fourier4") %>%
add_trace(y = Modelo_serie_diff_models$DummyDesempleoDiff, mode = 'lines', line = list(color = 'purple'), name = "Dummy") %>%  
  layout(title = 'Tasa de Desempleo sin tendencia (Diferencia Ordinaria, lag=1)',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))


```

#### Extracción

```{r}
plot_ly(x = time(DiferenciaOrd), y = DiferenciaOrd, type = 'scatter', mode = 'lines', color = I("red"), name = "SerieDifOrd") %>%
add_trace(y = DiferenciaOrd-Modelo_serie_diff_models$Fourier1DesempleoDiff, mode = 'lines', line = list(color = 'blue'), name = "Fourier1") %>%
add_trace(y = DiferenciaOrd-Modelo_serie_diff_models$Fourier2DesempleoDiff, mode = 'lines', line = list(color = 'green'), name = "Fourier2") %>%
add_trace(y = DiferenciaOrd-Modelo_serie_diff_models$Fourier3DesempleoDiff, mode = 'lines', line = list(color = 'yellow'), name = "Fourier3") %>%
add_trace(y = DiferenciaOrd-Modelo_serie_diff_models$Fourier4DesempleoDiff, mode = 'lines', line = list(color = '#00CD66'), name = "Fourier4") %>%
add_trace(y = DiferenciaOrd-Modelo_serie_diff_models$DummyDesempleoDiff, mode = 'lines', line = list(color = 'purple'), name = "Dummy") %>%  
  layout(title = 'Tasa de Desempleo sin tendencia y estacionalidad (Diferencia Ordinaria, lag=1)',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))

```

```{r}
par(mfrow = c(2, 2))
acf(DiferenciaOrd-Modelo_serie_diff_models$Fourier2DesempleoDiff,lag.max =100,main="Autocorrelación serie sin componentes\n(F2)")
acf(DiferenciaOrd-Modelo_serie_diff_models$Fourier3DesempleoDiff,lag.max =100,main="Autocorrelación serie sin componentes\n(F3)")
acf(DiferenciaOrd-Modelo_serie_diff_models$Fourier4DesempleoDiff,lag.max =100,main="Autocorrelación serie sin componentes\n(F4)")
acf( DiferenciaOrd-Modelo_serie_diff_models$DummyDesempleoDiff,lag.max =100,main="Autocorrelación serie sin componentes\n(Dummy)")
```

## Suavizamiento Exponencial

El suavizamiento exponencial es un modelo que contiene 3 parámetros, dos relacionados a la estimación de la componente de tendencia, y el tercero a la componente de estacionalidad.

```{=tex}
\begin{align*}
a_t & = \alpha(x_t-s_{t-p}) + (1-\alpha)(a_{t-1}+b_{t-1}) \\
b_t & = \beta(a_t-a_{t-1}) + (1-\beta)b_{t-1} \\
s_t & = \gamma(x_t-a_t) + (1-\gamma)s_{t-p}
\end{align*}
```
El siguiente modelo tendrá las tres componentes, por lo tanto, para evaluar la capacidad predictiva del modelo, utilizando la predicción a un paso, emplearemos el 80% de la serie como conjunto de entrenamiento y el 20% restante como prueba. Es decir, el periodo de entrenamiento corresponde a los primeros 221 meses, es decir, de enero de 2001 hasta mayo de 2019, por otro lado, el periodo de prueba corresponde a junio de 2019 hasta diciembre de 2023. Recordemos que trabajaremos la serie original.

```{r}
Train=ts(DesempleoTS[1:221], start = c(2001, 1),frequency = 12)
Test=ts(DesempleoTS[222:276],start=c(2019,6),frequency = 12)
```

#### Rolling

##### Rolling manual

```{r Rolling 1}
h=1

lserie=length(DesempleoTS)
ntrain=trunc(length(DesempleoTS)*0.80)+1 ##% del datos en el conjunto de entrenamiento es del 80%.
ntrain
time(DesempleoTS)
time(DesempleoTS)[ntrain]###Me entrega la ultima fecha de la posición ntrain
#Partiendo la serie en entrenamiento y test
train=window(DesempleoTS,end=time(DesempleoTS)[ntrain])
test=window(DesempleoTS,start=time(DesempleoTS)[ntrain]+1/12)##1/12 porque es la fracción que corresponde a un mes
length(train)
ntest=length(test)
ntest ##Me define el valor de origins, o de ventanas de rolling.
lserie ### Comparar los valores
fchstepahe=matrix(0,nrow=ntest,ncol=h) ##Crea una Columna para los h-pasos adelante
### verval contiene los verdaderos valores de la serie en el conjunto de prueba con los que se compararán los pronósticos.
verval=cbind(test[1:ntest])
#for(j in 2:h){
 # verval=cbind(verval,c(test[j:ntest],rep(NA,j-1))) ##### Este for sobra porque sólo hacemos predicción un paso adelante
#}

#verval=cbind(test[1:ntest],c(test[2:ntest],NA),c(test[3:ntest],NA,NA))
####Ajuste del modelo con los datos de entrenamiento
HWAP_train=stats::HoltWinters(train,seasonal="additive")
HWAP_train$alpha
HWAP_train$beta
HWAP_train$gamma
###Observación: Note que que esos son las estimaciones de los parámetros de suavizamiento. Se puede también hacer una grilla de valores para explorar si hay unos valores mejores.
# por ejemplo como sigue:
require(utils)
suav_inputs=cbind(seq(0.001,0.999,0.1),seq(0.001,0.999,0.1),seq(0.001,0.999,0.1))
colnames(suav_inputs)<-c("alpha","beta","gamma")
suav_inputs_tbl=tibble::as_tibble(suav_inputs)
grilla_suav=expand.grid(alpha=suav_inputs_tbl$alpha,beta=suav_inputs_tbl$beta,gamma=suav_inputs_tbl$gamma) ##Grilla de Valores
####Se crean las ventanas de rolling y se obtiene los h-pronósticos para cada ventana(hay ntest posibles ventanas)
for(i in 1:(ntest)){
  x=window(DesempleoTS,end=time(DesempleoTS)[ntrain]+(i-1)/12)
  print(length(x))
  refit=stats::HoltWinters(x,seasonal="additive",alpha=HWAP_train$alpha,beta=HWAP_train$beta,gamma=HWAP_train$gamma)
    fchstepahe[i,]=as.numeric(forecast::forecast(refit,h=h)$mean)
}
fchstepahe 
errores_pred=verval -fchstepahe ##Observación: debo devolver los pronósticos y los verdaderos valores a la escala original si es necesario.
ECM=apply(errores_pred^2,MARGIN = 2,mean,na.rm=TRUE) ##Acá se computa la medida de precisión del pronóstico(en este caso ECM).
RECM=sqrt(ECM) ##Se le saca raíz 
#RECM ##se lee: Primera fila RECM 1-paso adelante y así sucesivamente.
ECM
```

```{r}
plot_ly(x = time(Test), y = Test, type = 'scatter', mode = 'lines', color = I("red"), name = "TasaDesempleo") %>%
  add_trace(y = fchstepahe, mode = 'lines', line = list(color = 'blue'), name = "Predicciones") %>%
  layout(title = 'Tasa de Desempleo vs Predicciones (Junio 2019-Diciembre 2023)',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))
```

##### Rolling usando la función

```{r rolling 2}
library(forecast)
library(greybox)
HWAP_train=stats::HoltWinters(train,seasonal="additive")

h=1
ourCallETS <- "forecast::forecast(stats::HoltWinters(x=data,alpha=HWAP_train$alpha,beta=HWAP_train$beta,gamma=HWAP_train$gamma),h=h,level=95)"
###Note que x=data es solo un argumento indeterminado.
ourValueETS <- c("mean","lower","upper")
origins=ntest   ##número de rolling windows
Valoresretornados1 <- ro(DesempleoTS, h=h, origins=origins, call=ourCallETS, value=ourValueETS,ci=FALSE,co=FALSE)
t(Valoresretornados1$holdout)## Permiten verificar los verdaderos valores h-pasos adelante. 
t(Valoresretornados1$mean)
apply((Valoresretornados1$holdout -Valoresretornados1$mean)^2,1,mean,na.rm=TRUE) ### Se calcula el error cuadrático medio de predicción

```

Veamos si el error cuadrático medio haciendolo manualmente es igual a cuando utilizamos la función.

```{r}
apply((Valoresretornados1$holdout -Valoresretornados1$mean)^2,1,mean,na.rm=TRUE) == ECM
```

El error cuadrático medio de las predicciones un paso adelante tanto en el rolling manual como utilizando la función son iguales.

## Ajuste modelo ARMA

### Identificación del Modelo Usando ACF y PACF

Primero, para ajustar este modelo, se utilizó la serie con diferenciación ordinaria y además removiendo su componente estacionaria ajustada a través de variables dummy.

```{r}
DesempNSDiff <- DiferenciaOrd-Modelo_serie_diff_models$DummyDesempleoDiff
plot(DesempNSDiff)
acf(DesempNSDiff,lag.max = length(DesempNSDiff))
acf(DesempNSDiff,ci.type='ma',lag.max = length(DesempNSDiff)/4) ## Rezago máximo q = 1
pacf(DesempNSDiff,lag.max = length(DesempNSDiff)/4) ## Rezago máximo p = 1
```

Viendo esto, se pueden proponer 3 modelos, AR(1), MA(1) y ARMA(1,1).

### Estimación del modelo

```{r Estimacion}
library(lmtest)
ARPURODesem=Arima(DesempNSDiff,order=c(1,0,0),include.mean = FALSE)
MAPURODesem=Arima(DesempNSDiff,order=c(0,0,1),include.mean = FALSE)
ARMAMIXTODesem=Arima(DesempNSDiff,order=c(1,0,1),include.mean = FALSE)
coeftest(ARPURODesem)
coeftest(MAPURODesem)
coeftest(ARMAMIXTODesem)
summary(ARPURODesem)
```

Se hizo un ARMA automático por si lo visto visualmente en el ACF y PACF fue malinterpretado, sin embargo llega a la misma conclusión, un AR(1) es el mejor entre las propuestas.

```{r}
Arima_automatico_desempleo=forecast::auto.arima(DesempNSDiff,d=0,D=0,max.p=5,max.q=5,start.p=0, start.q=0,seasonal=FALSE,max.order=10,stationary=TRUE,ic="bic",stepwise=FALSE,allowmean = TRUE) 

Arima_automatico_desempleo 
```

Vamos a continuar con el AR(1), veamos si se verifican los supuestos.

### Análisis de residuales

```{r}
# An?lisis de residuales
residualesARPURO=ARPURODesem$residuals
plot(residualesARPURO)
acf(residualesARPURO)
#acf(residualesARPURO^2)
pacf(residualesARPURO)

#Test de autocorrelaci?n
length(residualesARPURO)/4
sqrt(length(residualesARPURO))
Box.test(residualesARPURO, lag =30 , type = "Ljung-Box", fitdf = 1) #No puedo Rechazar la hipótesis de no autocorrelación!

#Test de normalidad
tseries::jarque.bera.test(residualesARPURO) # Se rechaza la hipótesis nula de normalidad, el modelo no es adecuado



###Estad?sticas CUSUM
resARPURO=residualesARPURO
cum=cumsum(resARPURO)/sd(resARPURO)
N=length(resARPURO)
cumq=cumsum(resARPURO^2)/sum(resARPURO^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.09821####Valor del cuantil aproximado para cusumsq para n/2 aprox 130
LS=Af*sqrt(N)+2*Af*c(1:length(resARPURO))/sqrt(N)
LI=-LS
LQS=co+(1:length(resARPURO))/N
LQI=-co+(1:length(resARPURO))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUMSQ
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")
```

Entonces, un modelo AR(1), y en general, un modelo ARMA no es el adecuado para modelar esta serie de tiempo, por lo que no se realizarán predicciones.

## Ajuste modelo ARIMA

### Prueba de Raíz Unitaria

Ahora avanzamos en el setido de verificar si la serie muestra la presencia de una o varias raíces unitarias

```{r}
acf(DesempleoTS,lag.max = 60)
pacf(DesempleoTS)
ar(DesempleoTS) ##Selecciona un modelo AR usando el crierio de Akaike  a la serie Aaa



aTSA::adf.test(DesempleoTS,nlag = 14) #Toca usar el rezago que da ar() + 1
summary(ur.df(DesempleoTS,type="none",lags = 12))
summary(ur.df(DesempleoTS,type="drift",lags = 12)) # Se utiliza lags = 12 porque ahí el último retardo es significativo



######SE debe chequear si hay que diferenciar de nuevo!!!
plot(DiferenciaOrd)
#####Transformación requrida para los datos(transformación Box-Cox y diferencia ordinaria)
pacf(DiferenciaOrd)
acf(DiferenciaOrd,lag.max = 48)
ar(DiferenciaOrd)


aTSA::adf.test(DiferenciaOrd,nlag = 25)
summary(ur.df(DiferenciaOrd,type="none",lags = 23))
monthplot(DiferenciaOrd)
spectrum(DiferenciaOrd)

```

Sin drift ni tendencia en el rezago 24 no hay raíz unitaria y con la prueba se ve que ese rezago es significativo por lo que no se diferenciará más la serie.

### Identificación del Modelo Usando ACF y PACF

```{r}
####Identificación de los Órdenes Autoregresivos
acf(DiferenciaOrd,lag.max = 120,ci.type='ma')  ###Se requiere un MA de orden muy grande
pacf(DiferenciaOrd,lag.max = 60)  ####Puede ser un autoregresivo de orden 12

###Arima Automático
modelo.automatico1=auto.arima(DiferenciaOrd,d=0,D=0,max.p=24,max.q=60,start.p=0, start.q=0,seasonal=FALSE,max.order=24,stationary=TRUE,ic="aicc",stepwise=FALSE,allowmean = TRUE)
modelo.automatico1
```

### Estimación del modelo

```{r}
#####Ajuste del Modelo
####Note que entramos la serie original
AjusteArima151=forecast::Arima(DesempleoTS,order = c(15,1,1),lambda = 1)
summary(AjusteArima151)
coeftest(AjusteArima151)
#####Refinando el modelo
AjusteArimaRef=forecast::Arima(DesempleoTS,order = c(15,1,1),lambda = 1, fixed=c(NA,NA,NA,NA,NA,0,0,NA,0,NA,NA,NA,NA,0,NA,NA))

summary(AjusteArimaRef)
coeftest(AjusteArimaRef)

```

### Análisis de Residuales

```{r}
residualesARIMA=AjusteArimaRef$residuals
plot(residualesARIMA)
#
#plot(SDresiduales)
acf(residualesARIMA,lag.max = 48)
pacf(residualesARIMA)

#acf(SDresiduales)
#pacf(SDresiduales)


#Test de normalidad
jarque.bera.test(residualesARIMA)
#Test de autocorrelación
length(residualesARIMA)/4
sqrt(length(residualesARIMA))
Box.test(residualesARIMA, lag =30 , type = "Ljung-Box", fitdf = 12)



monthplot(residualesARIMA)
spectrum(residualesARIMA,spans = c(3,5))
###Estad?sticas CUSUM
resARIMA=residualesARIMA
cum=cumsum(resARIMA)/sd(resARIMA)
N=length(resARIMA)
cumq=cumsum(resARIMA^2)/sum(resARIMA^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.09821
LS=Af*sqrt(N)+2*Af*c(1:length(resARIMA))/sqrt(N)
LI=-LS
LQS=co+(1:length(resARIMA))/N
LQI=-co+(1:length(resARIMA))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUMSQ
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")


#####Fase de Pronósticos
pronosticos151=forecast::forecast(AjusteArimaRef,h=12,level=0.95)
plot(pronosticos151)
```

### Ajuste de la estacionalidad con Dummys

```{r ajuste de la estacionalidad con componentes Dummy}
tsibbleDesem<-as_tsibble(DesempleoTS)

#####Modelo

###Armónicos

ajuste_dummy<-tsibbleDesem%>%model(
  `Dummy`=ARIMA(value~0+season()+pdq(15,1,1)+PDQ(0,0,0)))

glance(ajuste_dummy)

ajuste_dummy %>%
     select(Dummy)%>%coef()

ajuste_dummy %>%
  fabletools::forecast(h = "2 years") %>%
  autoplot(tsibbleDesem, level = 95) +  facet_wrap(vars(.model), ncol = 2) +
  guides(colour = "none", fill = "none", level = "none") +
  labs(title= "Tasa de desempleo"
      )

```

## Ajuste modelo SARIMA

Debido al análisis previo, vemos que hay una componente estacional que puede ser estimada, por lo que se ajustará un modelo SARIMA sin drift debido a que se diferenciará 2 veces, una diferencia ordinaria y una diferencia estacional.

### Identificación del Modelo Usando ACF y PACF

```{r}
####Diferencia Ordinaria############
nsdiffs(DiferenciaOrd)
DdDesempleoTS=diff(DiferenciaOrd,lag=12)###lag=s

plot(DiferenciaOrd)
plot(DdDesempleoTS)
monthplot(DdDesempleoTS)
acf(DdDesempleoTS,lag.max = 36)
spectrum(DdDesempleoTS)
nsdiffs(DdDesempleoTS)


##Autocorrelogramas
acf(DdDesempleoTS)
acf(DdDesempleoTS,lag.max = 48, ci.type='ma')# q=0, Q=1
pacf(DdDesempleoTS,lag.max = 120) # p=0, P=5
#En resumen podríamos proponer max q=0, Q=1, p=0, P=5
#SARIMA(p=0,d=1,q=0)x(P=5,D=1,Q=1)s=12 sin drift
```

### Estimación del modelo

```{r}
##Ajuste del modelo
###Arima Estacional


#Modelo
modeloSARIMA = Arima(DesempleoTS, c(0, 1, 0),seasonal = list(order = c(5, 1, 1), period = 12),lambda = 1)

coeftest(modeloSARIMA)
```

### Análisis de residuales

```{r}
## Análisis de residuales

residualesSARIMA <- modeloSARIMA$residuals
plot(residualesSARIMA)
acf(residualesSARIMA,lag.max = 50)
pacf(residualesSARIMA,lag.max = 50)
#Test de autocorrelaci?n
Box.test(residualesSARIMA, lag = (length(residualesSARIMA)/4), type = "Ljung-Box", fitdf = 5)
######Análisis de Outliers
#Test de normalidad
jarque.bera.test(residualesSARIMA)



###Estad?ticas CUSUM
resSARIMA=residualesSARIMA
cum=cumsum(resSARIMA)/sd(resSARIMA)
N=length(resSARIMA)
cumq=cumsum(resSARIMA^2)/sum(resSARIMA^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.09821####Valor del cuantil aproximado para cusumsq para 140
LS=Af*sqrt(N)+2*Af*c(1:length(resSARIMA))/sqrt(N)
LI=-LS
LQS=co+(1:length(resSARIMA))/N
LQI=-co+(1:length(resSARIMA))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUM Square
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")

```

## Detección e intervención de Outliers

### Detección del outliers

```{r}
n=length(DesempleoTS)
coef=coefs2poly(modeloSARIMA)
coef
outliers= tsoutliers::locate.outliers(resSARIMA,coef)
outliers###tstat se compara con C=3
xreg = outliers.effects(outliers, n)

SARIMA_out = update(modeloSARIMA, xreg = xreg)
```

### SARIMA(0,1,0)(5,1,1)

```{r}
## Análisis de residuales

residualesSARIMA <- SARIMA_out$residuals
plot(residualesSARIMA)
acf(residualesSARIMA,lag.max = 50)
pacf(residualesSARIMA,lag.max = 50)
#Test de autocorrelaci?n
Box.test(residualesSARIMA, lag = (length(residualesSARIMA)/4), type = "Ljung-Box", fitdf = 12)
######Análisis de Outliers
#Test de normalidad
jarque.bera.test(residualesSARIMA)



###Estad?ticas CUSUM
resSARIMA=residualesSARIMA
cum=cumsum(resSARIMA)/sd(resSARIMA)
N=length(resSARIMA)
cumq=cumsum(resSARIMA^2)/sum(resSARIMA^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.09821####Valor del cuantil aproximado para cusumsq para 140
LS=Af*sqrt(N)+2*Af*c(1:length(resSARIMA))/sqrt(N)
LI=-LS
LQS=co+(1:length(resSARIMA))/N
LQI=-co+(1:length(resSARIMA))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUM Square
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")

```

### Detección automática de outliers

```{r}
n=length(DesempleoTS)
salida_tso = tso(DesempleoTS, maxit.oloop = 10)
plot(salida_tso)
xreg = outliers.effects(salida_tso$outliers, n)
salida_tso

SARIMA_out = Arima(DesempleoTS, c(2, 1, 0),seasonal = list(order = c(2, 1, 0), period = 12),lambda = 1, xreg = xreg)
```

### SARIMA(2,1,0)(2,1,0)

```{r}
## Análisis de residuales

residuales <- SARIMA_out$residuals
plot(residuales)
acf(residuales,lag.max = 50)
pacf(residuales,lag.max = 50)
#Test de autocorrelaci?n
Box.test(residuales, lag = (length(residualesSARIMA)/4), type = "Ljung-Box", fitdf = 10)
######Análisis de Outliers
#Test de normalidad
jarque.bera.test(residualesSARIMA)



###Estad?ticas CUSUM
res=residualesSARIMA
cum=cumsum(res)/sd(res)
N=length(res)
cumq=cumsum(res^2)/sum(res^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.09821####Valor del cuantil aproximado para cusumsq para 140
LS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)
LI=-LS
LQS=co+(1:length(res))/N
LQI=-co+(1:length(res))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUM Square
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")

```

Este modelo no logra pasar todos los supuestos pues parece haber autocorrelación no explicada, por lo que se aumentará el orden de q y Q para ver si se soluciona este problema.

### SARIMA(2,1,1)(2,1,1)

```{r}
modeloSARIMA = Arima(DesempleoTS, c(2, 1, 1),seasonal = list(order = c(2, 1, 1), period = 12),lambda = 1)
resSARIMA <- modeloSARIMA$residuals
n=length(DesempleoTS)
coef=coefs2poly(modeloSARIMA)
coef
outliers= tsoutliers::locate.outliers(resSARIMA,coef)
outliers###tstat se compara con C=3
xreg = outliers.effects(outliers, n)

SARIMA_out = update(modeloSARIMA, xreg = xreg)
SARIMA_out
```

```{r}
## Análisis de residuales

residuales <- SARIMA_out$residuals
plot(residuales)
acf(residuales,lag.max = 50)
pacf(residuales,lag.max = 50)
#Test de autocorrelaci?n
Box.test(residuales, lag = (length(residuales)/4), type = "Ljung-Box", fitdf = 16)
######Análisis de Outliers
#Test de normalidad
jarque.bera.test(residuales)



###Estad?ticas CUSUM
res=residuales
cum=cumsum(res)/sd(res)
N=length(res)
cumq=cumsum(res^2)/sum(res^2)
Af=0.948 ###Cuantil del 95% para la estad?stica cusum
co=0.09821####Valor del cuantil aproximado para cusumsq para 140
LS=Af*sqrt(N)+2*Af*c(1:length(res))/sqrt(N)
LI=-LS
LQS=co+(1:length(res))/N
LQI=-co+(1:length(res))/N
plot(cum,type="l",ylim=c(min(LI),max(LS)),xlab="t",ylab="",main="CUSUM")
lines(LS,type="S",col="red")
lines(LI,type="S",col="red")
#CUSUM Square
plot(cumq,type="l",xlab="t",ylab="",main="CUSUMSQ")                      
lines(LQS,type="S",col="red")                                                                           
lines(LQI,type="S",col="red")

```

Este modelo logró pasar todos los supuestos, se revisará si hay algún outlier en este modelo.

```{r}
SARIMA_out

resFINAL <- SARIMA_out$residuals
n=length(DesempleoTS)
coefFINAL=coefs2poly(SARIMA_out)
coefFINAL
outliersFINAL= locate.outliers(resFINAL,coefFINAL)
outliersFINAL
```

Ya no hay más outliers, por lo que se procederá a evaluar su ECM de la predicción un paso adelante haciendo rolling.

## Rolling modelo SARIMA final

```{r}
SARIMA_out
#refit <- Arima(DesempleoTS, model=fitmodelo)
fc <- window(fitted(SARIMA_out), start=c(2019,6))

esta_param_modelo<-coef(SARIMA_out)
h <- 1
train = window(DesempleoTS,end=time(DesempleoTS)[ntrain])
test = window(DesempleoTS,start=time(DesempleoTS)[ntrain]+1/12)
n <- length(test) - h + 1
fitmodelo <- update(SARIMA_out,fixed=esta_param_modelo)
fc <- ts(numeric(n), start=c(2019,6), freq=12)
for(i in 1:n)
{ 
  x <- window(DesempleoTS, end=c(2019, 5+(i-1)))
  refit <- forecast::Arima(x, model=fitmodelo, xreg = xreg[1:(221+(i-1)),])
  fc[i] <- forecast::forecast(refit, h=h,xreg = t(as.matrix(xreg[(221+i),])))$mean[h]
}
dife=(test-fc)^2
ecm=(1/(length(test)))*sum(dife)
ecm
```

```{r}
plot_ly(x = time(test), y = test, type = 'scatter', mode = 'lines', color = I("red"), name = "TasaDesempleo") %>%
  add_trace(y = fc, mode = 'lines', line = list(color = 'blue'), name = "Predicciones") %>%
  layout(title = 'Tasa de Desempleo vs Predicciones (Junio 2019-Diciembre 2023)',
         xaxis = list(title = 'Mes', rangeslider = list(type = 'date')))
```
